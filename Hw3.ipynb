{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hypothesis(theta, X):\n",
    "    \"\"\"\n",
    "    Функція гіпотези лінійної регресії у векторному вигляді\n",
    "    \n",
    "    Параметри:\n",
    "    theta -- вектор параметрів моделі [θ₀, θ₁]\n",
    "    X -- вхідний вектор або матриця, де кожен рядок є точкою даних\n",
    "    \n",
    "    Повертає:\n",
    "    вектор прогнозованих значень\n",
    "    \"\"\"\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "def cost_function(theta, X, y):\n",
    "    \"\"\"\n",
    "    Функція втрат у векторному вигляді\n",
    "    \n",
    "    Параметри:\n",
    "    theta -- вектор параметрів моделі [θ₀, θ₁]\n",
    "    X -- вхідний вектор або матриця, де кожен рядок є точкою даних\n",
    "    y -- вектор фактичних значень\n",
    "    \n",
    "    Повертає:\n",
    "    значення функції втрат\n",
    "    \"\"\"\n",
    "    m = len(y)  # кількість точок даних\n",
    "    predictions = np.dot(X, theta)  # прогнозування\n",
    "    sq_errors = (predictions - y) ** 2  # квадратичні помилки\n",
    "    return 1 / (2 * m) * np.sum(sq_errors)  # середнє значення квадратичних помилок\n",
    "\n",
    "def gradient_descent_step(theta, X, y, alpha):\n",
    "    \"\"\"\n",
    "    Один крок градієнтного спуску\n",
    "    \n",
    "    Параметри:\n",
    "    theta -- вектор параметрів моделі [θ₀, θ₁]\n",
    "    X -- вхідний вектор або матриця, де кожен рядок є точкою даних\n",
    "    y -- вектор фактичних значень\n",
    "    alpha -- швидкість навчання (learning rate)\n",
    "    \n",
    "    Повертає:\n",
    "    оновлений вектор параметрів моделі\n",
    "    \"\"\"\n",
    "    m = len(y)  # кількість точок даних\n",
    "    predictions = np.dot(X, theta)  # прогнозування\n",
    "    error = predictions - y  # помилки прогнозування\n",
    "    gradient = np.dot(X.T, error)  # градієнт функції втрат\n",
    "    theta -= alpha * (1 / m) * gradient  # оновлення параметрів\n",
    "    return theta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найкращі знайдені параметри: [nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IJHEA\\AppData\\Local\\Temp\\ipykernel_9088\\3744584828.py:21: RuntimeWarning: overflow encountered in square\n",
      "  sq_errors = (predictions - y) ** 2\n",
      "C:\\Users\\IJHEA\\AppData\\Local\\Temp\\ipykernel_9088\\3744584828.py:32: RuntimeWarning: invalid value encountered in subtract\n",
      "  theta -= alpha * (1 / m) * gradient\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Завантаження датасету\n",
    "df = pd.read_csv('C:\\\\Users\\\\IJHEA\\\\Documents\\\\Python\\\\HWDS-Rizun\\\\Housing.csv')\n",
    "\n",
    "# Підготовка даних\n",
    "X = df[['area', 'bathrooms', 'bedrooms']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Додавання одиниці для параметру w0 (константа)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# Ініціалізація параметрів моделі\n",
    "theta = np.zeros(X.shape[1])\n",
    "\n",
    "# Функція втрат\n",
    "def cost_function(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions = np.dot(X, theta)\n",
    "    sq_errors = (predictions - y) ** 2\n",
    "    return 1 / (2 * m) * np.sum(sq_errors)\n",
    "\n",
    "# Градиєнтний спуск\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    costs = []\n",
    "    for _ in range(iterations):\n",
    "        predictions = np.dot(X, theta)\n",
    "        error = predictions - y\n",
    "        gradient = np.dot(X.T, error)\n",
    "        theta -= alpha * (1 / m) * gradient\n",
    "        cost = cost_function(X, y, theta)\n",
    "        costs.append(cost)\n",
    "    return theta, costs\n",
    "\n",
    "# Запуск градієнтного спуску\n",
    "alpha = 0.01  # Швидкість навчання\n",
    "iterations = 1500  # Кількість ітерацій\n",
    "theta, costs = gradient_descent(X, y, theta, alpha, iterations)\n",
    "\n",
    "# Виведення знайдених параметрів\n",
    "print('Найкращі знайдені параметри:', theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найкращі знайдені параметри МНК: [-1.73171608e+05  3.78762754e+02  1.38604950e+06  4.06820034e+05]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Завантаження датасету\n",
    "df = pd.read_csv('C:\\\\Users\\\\IJHEA\\\\Documents\\\\Python\\\\HWDS-Rizun\\\\Housing.csv')\n",
    "\n",
    "# Підготовка даних\n",
    "X = df[['area', 'bathrooms', 'bedrooms']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Додавання одиниці для параметру w0 (константа)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# Знаходження параметрів за допомогою МНК\n",
    "XT_X = np.dot(X.T, X)\n",
    "XT_X_inv = np.linalg.inv(XT_X)\n",
    "XT_y = np.dot(X.T, y)\n",
    "w = np.dot(XT_X_inv, XT_y)\n",
    "\n",
    "# Виведення знайдених параметрів\n",
    "print('Найкращі знайдені параметри МНК:', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найкращі знайдені параметри градієнтного спуску: [nan nan nan nan]\n",
      "Найкращі знайдені параметри МНК: [-1.73171608e+05  3.78762754e+02  1.38604950e+06  4.06820034e+05]\n",
      "\n",
      "Різниця між параметрами градієнтного спуску та МНК: [nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IJHEA\\AppData\\Local\\Temp\\ipykernel_9088\\3330927563.py:31: RuntimeWarning: invalid value encountered in subtract\n",
      "  theta -= alpha * (1 / m) * gradient\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Завантаження датасету\n",
    "df = pd.read_csv('C:\\\\Users\\\\IJHEA\\\\Documents\\\\Python\\\\HWDS-Rizun\\\\Housing.csv')\n",
    "\n",
    "# Підготовка даних\n",
    "X = df[['area', 'bathrooms', 'bedrooms']].values\n",
    "y = df['price'].values\n",
    "\n",
    "# Додавання одиниці для параметру w0 (константа)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "# Ініціалізація параметрів моделі\n",
    "theta = np.zeros(X.shape[1])\n",
    "\n",
    "# Функція втрат\n",
    "def cost_function(X, y, theta):\n",
    "    m = len(y)\n",
    "    predictions = np.dot(X, theta)\n",
    "    sq_errors = (predictions - y) ** 2\n",
    "    return 1 / (2 * m) * np.sum(sq_errors)\n",
    "\n",
    "# Градиєнтний спуск\n",
    "def gradient_descent(X, y, theta, alpha, iterations):\n",
    "    m = len(y)\n",
    "    for _ in range(iterations):\n",
    "        predictions = np.dot(X, theta)\n",
    "        error = predictions - y\n",
    "        gradient = np.dot(X.T, error)\n",
    "        theta -= alpha * (1 / m) * gradient\n",
    "    return theta\n",
    "\n",
    "# Запуск градієнтного спуску\n",
    "alpha = 0.01  # Швидкість навчання\n",
    "iterations = 1500  # Кількість ітерацій\n",
    "theta_gd = gradient_descent(X, y, theta, alpha, iterations)\n",
    "\n",
    "# Виведення знайдених параметрів градієнтного спуску\n",
    "print('Найкращі знайдені параметри градієнтного спуску:', theta_gd)\n",
    "\n",
    "# Знаходження параметрів за допомогою МНК\n",
    "XT_X = np.dot(X.T, X)\n",
    "XT_X_inv = np.linalg.inv(XT_X)\n",
    "XT_y = np.dot(X.T, y)\n",
    "w_mnk = np.dot(XT_X_inv, XT_y)\n",
    "\n",
    "# Виведення знайдених параметрів МНК\n",
    "print('Найкращі знайдені параметри МНК:', w_mnk)\n",
    "\n",
    "# Порівняння результатів\n",
    "print('\\nРізниця між параметрами градієнтного спуску та МНК:', theta_gd - w_mnk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
